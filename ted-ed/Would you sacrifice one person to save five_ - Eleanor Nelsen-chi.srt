1
00:00:06,949 --> 00:00:11,521
Imagine you're watching a runaway trolley /ˈtrɒlɪ  电车/
barreling /ˈbærəl 桶/ down the tracks

2
00:00:11,521 --> 00:00:15,961
straight towards five workers
who can't escape /ɪˈskeɪp/.

3
00:00:15,961 --> 00:00:18,179
You happen to be standing next to a switch

4
00:00:18,179 --> 00:00:21,680
that will divert /daɪˈvɜːt 使转向/ the trolley /ˈtrɒlɪ  电车/
onto a second track.

5
00:00:21,680 --> 00:00:22,980
Here's the problem.

6
00:00:22,980 --> 00:00:28,069
That track has a worker on it, too,
but just one.

7
00:00:28,069 --> 00:00:29,390
What do you do?

8
00:00:29,390 --> 00:00:32,685
Do you sacrifice one person to save five?

9
00:00:32,685 --> 00:00:35,414
This is the trolley /ˈtrɒlɪ/ problem,

10
00:00:35,414 --> 00:00:42,103
a version of an ethical /ˈɛθɪkəl 伦理的 处方药/ dilemma /dɪˈlemə 困境/ that
philosopher /fɪˈlɒsəfə 哲学家/ Philippa Foot devised /dɪˈvaɪz  n 遗赠，遗赠的财产 v 设计，发明/ in 1967.

11
00:00:42,103 --> 00:00:45,371
It's popular because it forces us
to think about how to choose

12
00:00:45,371 --> 00:00:48,060
when there are no good choices.

13
00:00:48,060 --> 00:00:50,200
Do we pick the action 
with the best outcome

14
00:00:50,200 --> 00:00:55,400
or stick to a moral /ˈmɒrəl 道德/ code that prohibits
causing someone's death?

15
00:00:55,400 --> 00:01:00,837
In one survey, about 90% of respondents /rɪˈspɒndənt 回答者；被告/
said that it's okay to flip /flɪp 按（开关）/ the switch,

16
00:01:00,837 --> 00:01:04,250
letting one worker die to save five,

17
00:01:04,250 --> 00:01:08,600
and other studies, including a virtual /ˈvɜːtʃʊəl 实质上的；虚拟的/
reality /rɪˈælɪtɪ/ simulation of the dilemma /dɪˈlemə 困境/,

18
00:01:08,600 --> 00:01:11,040
have found similar results.

19
00:01:11,040 --> 00:01:16,061
These judgments are consistent with the
philosophical /ˌfɪləˈsɒfɪkəl/ principle /ˈprɪnsɪpəl ˈprɪnsəpəl 法则/ of utilitarianism /juːˌtɪlɪˈtɛərɪəˌnɪzəm 功利主义/

20
00:01:16,061 --> 00:01:18,521
which argues /ˈɑːɡjuː 争论/ that 
the morally /ˈmɒrəlɪ 道德上/ correct decision

21
00:01:18,521 --> 00:01:23,351
is the one that maximizes /ˈmæksɪˌmaɪz/ well-being
for the greatest number of people.

22
00:01:23,351 --> 00:01:25,481
The five lives outweigh /比…重要/ one,

23
00:01:25,481 --> 00:01:30,562
even if achieving that outcome requires
condemning /kənˈdɛm 谴责；宣判/ someone to death.

24
00:01:30,562 --> 00:01:33,471
But people don't always take
the utilitarian /juːˌtɪlɪˈtɛərɪən 功利主义者/ view,

25
00:01:33,471 --> 00:01:37,062
which we can see by changing
the trolley problem a bit.

26
00:01:37,062 --> 00:01:40,303
This time, you're standing on a bridge
over the track

27
00:01:40,303 --> 00:01:43,192
as the runaway trolley approaches.

28
00:01:43,192 --> 00:01:44,873
Now there's no second track,

29
00:01:44,873 --> 00:01:48,794
but there is a very large man 
on the bridge next to you.

30
00:01:48,794 --> 00:01:52,492
If you push him over, 
his body will stop the trolley,

31
00:01:52,492 --> 00:01:54,243
saving the five workers,

32
00:01:54,243 --> 00:01:56,033
but he'll die.

33
00:01:56,033 --> 00:01:59,432
To utilitarians /juːˌtɪlɪˈtɛərɪən 功利主义者/, 
the decision is exactly the same,

34
00:01:59,432 --> 00:02:01,982
lose one life to save five.

35
00:02:01,982 --> 00:02:04,584
But in this case, only about 10% of people

36
00:02:04,584 --> 00:02:08,453
say that it's OK to throw the man
onto the tracks.

37
00:02:08,453 --> 00:02:11,914
Our instincts /instiŋkt 本能；直觉/ tell us that deliberately /diˈlibəritli 故意地；从容不迫地/
causing someone's death

38
00:02:11,914 --> 00:02:16,303
is different than allowing them to die
as collateral /kɒˈlætərəl 担保品，抵押品 并行的/ damage.

39
00:02:16,303 --> 00:02:20,953
It just feels wrong for reasons
that are hard to explain.

40
00:02:20,953 --> 00:02:23,473
This intersection /ˌɪntəˈsɛkʃən 交叉点/ between ethics /ˈeθɪks 道德规范/
and psychology /saɪˈkɒlədʒɪ 心理学/

41
00:02:23,473 --> 00:02:26,604
is what's so interesting 
about the trolley problem.

42
00:02:26,604 --> 00:02:30,984
The dilemma /dɪˈlemə 困境/ in its many variations /ˌvɛərɪˈeɪʃən 不同版本/ reveal /rɪˈviːl 揭示/
that what we think is right or wrong

43
00:02:30,984 --> 00:02:36,345
depends on factors other than 
a logical weighing of the pros /prəʊ 赞成者 赞成票/ and cons /kɒn 反对票/.

44
00:02:36,345 --> 00:02:38,835
For example, men are more likely
than women

45
00:02:38,835 --> 00:02:42,504
to say it's okay to push the man
over the bridge.

46
00:02:42,504 --> 00:02:46,994
So are people who watch a comedy clip
before doing the thought experiment.

47
00:02:46,994 --> 00:02:49,165
And in one virtual /ˈvɜːtʃʊəl/ reality study,

48
00:02:49,165 --> 00:02:52,944
people were more willing 
to sacrifice men than women.

49
00:02:52,944 --> 00:02:55,214
Researchers have studied 
the brain activity

50
00:02:55,214 --> 00:02:59,535
of people thinking through the classic
and bridge versions.

51
00:02:59,535 --> 00:03:04,054
Both scenarios /sɪˈnɑːrɪˌəʊ 情节；剧本；方案/ activate areas of the brain
involved in conscious /ˈkɒnʃəs 意识到的/ decision-making

52
00:03:04,054 --> 00:03:06,514
and emotional responses.

53
00:03:06,514 --> 00:03:10,975
But in the bridge version,
the emotional response is much stronger.

54
00:03:10,975 --> 00:03:13,194
So is activity in an area of the brain

55
00:03:13,194 --> 00:03:16,884
associated with processing 
internal conflict.

56
00:03:16,884 --> 00:03:18,145
Why the difference?

57
00:03:18,145 --> 00:03:22,912
One explanation /ˌɛkspləˈneɪʃən 解释/ is that pushing someone
to their death feels more personal,

58
00:03:22,912 --> 00:03:26,925
activating an emotional aversion /əˈvɜːʃən 厌恶；讨厌的人或东西/
to killing another person,

59
00:03:26,925 --> 00:03:31,424
but we feel conflicted because we know
it's still the logical choice.

60
00:03:31,424 --> 00:03:36,405
"Trolleyology" has been criticized by some
philosophers and psychologists.

61
00:03:36,405 --> 00:03:41,266
They argue /ˈɑːɡjuː/ that it doesn't reveal anything
because its premise /ˈprɛmɪs 前提/ is so unrealistic /ˌʌnrɪəˈlɪstɪk/

62
00:03:41,266 --> 00:03:45,425
that study participants 
don't take it seriously.

63
00:03:45,425 --> 00:03:48,556
But new technology is making this kind
of ethical /ˈɛθɪkəl/ analysis

64
00:03:48,556 --> 00:03:50,698
more important than ever.

65
00:03:50,698 --> 00:03:54,036
For example, driver-less cars 
may have to handle choices

66
00:03:54,036 --> 00:03:58,007
like causing a small accident
to prevent a larger one.

67
00:03:58,007 --> 00:04:01,626
Meanwhile, governments are researching
autonomous /ɔːˈtɒnəməs 自治的/ military /ˈmɪlɪtərɪ/ drones /drəʊn/

68
00:04:01,626 --> 00:04:05,976
that could wind up / 最后落得/ making decisions of
whether they'll risk civilian /sɪˈvɪljən 平民/ casualties /ˈkæʒjʊəltɪ 意外事故；伤亡人员；受害者；急诊室/

69
00:04:05,976 --> 00:04:09,276
to attack a high-value target.

70
00:04:09,276 --> 00:04:11,197
If we want these actions to be ethical /ˈɛθɪkəl/,

71
00:04:11,197 --> 00:04:15,397
we have to decide in advance /ədˈvɑːns ədˈvæns美/
how to value human life

72
00:04:15,397 --> 00:04:17,666
and judge the greater good.

73
00:04:17,666 --> 00:04:20,106
So researchers who study 
autonomous /ɔːˈtɒnəməs 自治的/ systems

74
00:04:20,107 --> 00:04:22,207
are collaborating /kəˈlæbəˌreɪt 合作/ with philosophers

75
00:04:22,207 --> 00:04:27,628
to address the complex problem
of programming ethics into machines,

76
00:04:27,628 --> 00:04:30,957
which goes to show that 
even hypothetical /ˌhaɪpəˈθɛtɪkəl 假设的/ dilemmas /dɪˈlemə/

77
00:04:30,957 --> 00:00:00,000
can wind up on a collision /kəˈlɪʒən 相撞；冲突/ course
with the real world.

